var documenterSearchIndex = {"docs":
[{"location":"guide/#Guide","page":"Guide","title":"Guide","text":"","category":"section"},{"location":"guide/#Creating-learners","page":"Guide","title":"Creating learners","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"The following creates a two-action linear reward-penalty learner with global learning rate 0.001.","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"using LearningAutomata\nalice = LRPLearner(2, 0.001)","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"The second constructor argument may also be a vector. Here we create a  three-action learner with asymmetric learning rates for the different actions:","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"using LearningAutomata # hide\nbob = LRPLearner(3, [0.001, 0.1, 0.02])","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"The remaining fields are set using keyword arguments in the constructor.  For instance, to force a different learning rate vector for penalties:","category":"page"},{"location":"guide/","page":"Guide","title":"Guide","text":"using LearningAutomata # hide\ncarla = LRPLearner(3, 0.001, b = [0.1, 0.2, 0.01])","category":"page"},{"location":"guide/#Simulations-in-a-stationary-random-environment","page":"Guide","title":"Simulations in a stationary random environment","text":"","category":"section"},{"location":"guide/","page":"Guide","title":"Guide","text":"using LearningAutomata # hide\nusing Plots\nENV[\"GKSwstype\"] = \"100\" # hide\nalice = LRPLearner(3, 0.01)\nsre = [0.4, 0.2, 0.1]\nsimulate!(alice, 1000, sre) |> plot\nsavefig(\"simulation.svg\"); nothing # hide","category":"page"},{"location":"guide/#Reconfiguring-learners","page":"Guide","title":"Reconfiguring learners","text":"","category":"section"},{"location":"guide/#Games","page":"Guide","title":"Games","text":"","category":"section"},{"location":"api/#LearningAutomata.jl-API","page":"API","title":"LearningAutomata.jl API","text":"","category":"section"},{"location":"api/#Types","page":"API","title":"Types","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"LRPLearner","category":"page"},{"location":"api/#LearningAutomata.LRPLearner","page":"API","title":"LearningAutomata.LRPLearner","text":"LRPLearner(n::Int,\n           a::Vector{Float64},\n           b::Vector{Float64}, \n           c::Vector{Float64},\n           W::Vector{Float64},\n           A::Matrix{Float64},\n           R::Vector{Matrix{Float64}}, \n           P::Vector{Matrix{Float64}}) <: AbstractLinearLearner\n\nA linear reward-penalty learner.\n\nA general linear reward-penalty learner with n actions, learning rates a for rewards, learning rates b for punishments, action costs c, action probability vector W, vectors of reward and penalty operators R and P, and advantage matrix A.\n\nReferences\n\nBush and Mosteller FIXME Kauhanen FIXME Narendra and Thathachar FIXME\n\n\n\n\n\n","category":"type"},{"location":"api/#Constructors","page":"API","title":"Constructors","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"","category":"page"},{"location":"api/#Functions","page":"API","title":"Functions","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"get_probs\nreward!\npunish!\ninteract!\nsimulate!","category":"page"},{"location":"api/#LearningAutomata.get_probs","page":"API","title":"LearningAutomata.get_probs","text":"get_probs(x::AbstractLearner)\n\nReturn the current action probability vector.\n\n\n\n\n\n","category":"function"},{"location":"api/#LearningAutomata.reward!","page":"API","title":"LearningAutomata.reward!","text":"reward!(x::AbstractLinearLearner, i::Int)\n\nReward the ith action of an AbstractLinearLearner.\n\n\n\n\n\n","category":"function"},{"location":"api/#LearningAutomata.punish!","page":"API","title":"LearningAutomata.punish!","text":"punish!(x::AbstractLinearLearner, i::Int)\n\nPunish the ith action of an AbstractLinearLearner.\n\n\n\n\n\n","category":"function"},{"location":"api/#LearningAutomata.interact!","page":"API","title":"LearningAutomata.interact!","text":"interact!(x::AbstractLearner, y::AbstractLearner; symmetric = true)\n\nMake two learners x and y interact.\n\nInteraction is symmetric, i.e. both learners update their state, if symmetric = true. Else it is asymmetric, and only y updates its state.\n\n\n\n\n\n","category":"function"},{"location":"api/#LearningAutomata.simulate!","page":"API","title":"LearningAutomata.simulate!","text":"simulate!(x::AbstractLearner, iter::Int, c::Vector{Float64})\n\nSimulate a learner for iter iterations in a stationary random environment constituted by penalty probability vector c.\n\nReturns the trajectory as a m times n matrix, where m is the trajectory length (number of simulation iterations) and n is the learner's dimensionality (number of actions). This can be redirected e.g. to plot in order to visualize the learning trajectory.\n\n\n\n\n\n","category":"function"},{"location":"#LearningAutomata.jl-Documentation","page":"Home","title":"LearningAutomata.jl Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"FIXME","category":"page"}]
}
